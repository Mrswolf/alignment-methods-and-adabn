{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of the effectiveness of the training pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, copy, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import skorch\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.callbacks import (LRScheduler, EpochScoring, Checkpoint, Callback,\n",
    "                              TrainEndCheckpoint, LoadInitState, EarlyStopping)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from brainda.datasets import BNCI2014001\n",
    "\n",
    "from brainda.paradigms import MotorImagery\n",
    "from brainda.algorithms.utils.model_selection import (\n",
    "    set_random_seeds, generate_kfold_indices, match_kfold_indices)\n",
    "from brainda.algorithms.deep_learning import EEGNet, ShallowNet\n",
    "\n",
    "from braindecode.models import ShallowFBCSPNet, TIDNet, EEGNetv4\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU devices: 1\n",
      "Current pytorch device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = torch.device(\"cuda:{:d}\".format(device_id) if torch.cuda.is_available() else \"cpu\")\n",
    "if device != 'cpu':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(\"Available GPU devices: {}\".format(torch.cuda.device_count()))\n",
    "print(\"Current pytorch device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data without any preprocessing steps\n",
    "\n",
    "- epoch: [-0.5, 4]\n",
    "- srate: 250Hz\n",
    "- channels: 22\n",
    "- classes: left hand, right hand, feet, and tongue\n",
    "- current subject: 3\n",
    "\n",
    "Note: In our settings, `session_0` is `session_E` and `session_1` is `session_T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_hook(raw, caches, verbose=False):\n",
    "    # do nothing\n",
    "    return raw, caches\n",
    "\n",
    "srate = 250\n",
    "dataset = BNCI2014001()\n",
    "selected_channels = [\n",
    "    'FZ', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'P1', 'PZ', 'P2', 'POZ']\n",
    "start_t = dataset.events['left_hand'][1][0] - 0.5\n",
    "duration = 4.5 # seconds\n",
    "if start_t+ duration > dataset.events['left_hand'][1][1]:\n",
    "    print(\"Warning: the current dataset avaliable trial duration is not long enough.\")\n",
    "events = ['left_hand', 'right_hand', 'feet', 'tongue']\n",
    "paradigm = MotorImagery(\n",
    "    channels=selected_channels, \n",
    "    srate=srate, \n",
    "    intervals=[(start_t, start_t + duration)], \n",
    "    events=events)\n",
    "\n",
    "event_id = [dataset.events[e][0] for e in events]\n",
    "paradigm.register_raw_hook(raw_hook)\n",
    "X, y, meta = paradigm.get_data(dataset, subjects=[3], return_concat=True, verbose=False)\n",
    "y = label_encoder(y, event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braindecode training pipeline\n",
    "\n",
    "Adapted from https://github.com/braindecode/braindecode/blob/master/examples/plot_bcic_iv_2a_moabb_trial.py\n",
    "\n",
    "No bandpass filtering and exponential moving standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.3854\u001b[0m        \u001b[32m1.5341\u001b[0m            \u001b[35m0.3194\u001b[0m        \u001b[31m1.7365\u001b[0m  0.0006  0.6304\n",
      "      2            \u001b[36m0.4306\u001b[0m        \u001b[32m1.2407\u001b[0m            \u001b[35m0.3611\u001b[0m        \u001b[31m1.6086\u001b[0m  0.0006  0.1703\n",
      "      3            \u001b[36m0.5660\u001b[0m        \u001b[32m1.0748\u001b[0m            \u001b[35m0.3958\u001b[0m        \u001b[31m1.4128\u001b[0m  0.0006  0.1678\n",
      "      4            \u001b[36m0.7118\u001b[0m        \u001b[32m0.9865\u001b[0m            \u001b[35m0.4444\u001b[0m        \u001b[31m1.2122\u001b[0m  0.0006  0.1665\n",
      "      5            \u001b[36m0.7743\u001b[0m        \u001b[32m0.8446\u001b[0m            \u001b[35m0.4861\u001b[0m        \u001b[31m1.1604\u001b[0m  0.0006  0.1682\n",
      "      6            \u001b[36m0.8160\u001b[0m        \u001b[32m0.7972\u001b[0m            \u001b[35m0.4965\u001b[0m        \u001b[31m1.1211\u001b[0m  0.0006  0.1670\n",
      "      7            \u001b[36m0.8542\u001b[0m        \u001b[32m0.6772\u001b[0m            \u001b[35m0.5347\u001b[0m        \u001b[31m1.0755\u001b[0m  0.0006  0.1666\n",
      "      8            0.8507        \u001b[32m0.6072\u001b[0m            \u001b[35m0.5417\u001b[0m        \u001b[31m1.0613\u001b[0m  0.0006  0.1658\n",
      "      9            \u001b[36m0.8681\u001b[0m        \u001b[32m0.5328\u001b[0m            \u001b[35m0.5486\u001b[0m        \u001b[31m1.0503\u001b[0m  0.0006  0.1682\n",
      "     10            \u001b[36m0.8993\u001b[0m        \u001b[32m0.5160\u001b[0m            \u001b[35m0.5660\u001b[0m        \u001b[31m1.0188\u001b[0m  0.0006  0.1664\n",
      "     11            \u001b[36m0.9236\u001b[0m        \u001b[32m0.4686\u001b[0m            \u001b[35m0.5903\u001b[0m        \u001b[31m0.9965\u001b[0m  0.0006  0.1643\n",
      "     12            \u001b[36m0.9583\u001b[0m        \u001b[32m0.3961\u001b[0m            \u001b[35m0.6389\u001b[0m        \u001b[31m0.9625\u001b[0m  0.0006  0.1647\n",
      "     13            \u001b[36m0.9688\u001b[0m        \u001b[32m0.3827\u001b[0m            0.6250        \u001b[31m0.9326\u001b[0m  0.0006  0.1672\n",
      "     14            0.9618        \u001b[32m0.3656\u001b[0m            0.6215        \u001b[31m0.9248\u001b[0m  0.0006  0.1664\n",
      "     15            \u001b[36m0.9757\u001b[0m        \u001b[32m0.2938\u001b[0m            0.6354        0.9344  0.0006  0.1642\n",
      "     16            \u001b[36m0.9861\u001b[0m        0.3004            0.6215        \u001b[31m0.9116\u001b[0m  0.0006  0.1665\n",
      "     17            0.9792        \u001b[32m0.2614\u001b[0m            0.6389        \u001b[31m0.8880\u001b[0m  0.0006  0.1666\n",
      "     18            \u001b[36m0.9931\u001b[0m        0.2654            0.6285        0.9098  0.0006  0.1648\n",
      "     19            0.9896        \u001b[32m0.2008\u001b[0m            \u001b[35m0.6493\u001b[0m        0.9354  0.0006  0.1671\n",
      "     20            0.9931        \u001b[32m0.1929\u001b[0m            0.6389        0.9304  0.0006  0.1687\n",
      "     21            \u001b[36m0.9965\u001b[0m        \u001b[32m0.1916\u001b[0m            0.6354        0.8921  0.0006  0.1669\n",
      "     22            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1797\u001b[0m            \u001b[35m0.6528\u001b[0m        \u001b[31m0.8639\u001b[0m  0.0006  0.1659\n",
      "     23            0.9965        0.1880            \u001b[35m0.6632\u001b[0m        0.8826  0.0006  0.1657\n",
      "     24            0.9931        \u001b[32m0.1795\u001b[0m            0.6458        0.9457  0.0005  0.1683\n",
      "     25            0.9965        \u001b[32m0.1604\u001b[0m            0.6632        0.8658  0.0005  0.1670\n",
      "     26            1.0000        \u001b[32m0.1351\u001b[0m            \u001b[35m0.6736\u001b[0m        \u001b[31m0.8484\u001b[0m  0.0005  0.1672\n",
      "     27            1.0000        0.1365            \u001b[35m0.6806\u001b[0m        \u001b[31m0.8424\u001b[0m  0.0005  0.1686\n",
      "     28            0.9931        \u001b[32m0.1312\u001b[0m            \u001b[35m0.6910\u001b[0m        \u001b[31m0.8401\u001b[0m  0.0005  0.1669\n",
      "     29            1.0000        \u001b[32m0.1030\u001b[0m            \u001b[35m0.7083\u001b[0m        \u001b[31m0.8339\u001b[0m  0.0005  0.1677\n",
      "     30            1.0000        0.1249            0.6840        0.8404  0.0005  0.1665\n",
      "     31            1.0000        \u001b[32m0.0913\u001b[0m            0.6840        \u001b[31m0.8158\u001b[0m  0.0005  0.1647\n",
      "     32            1.0000        0.1066            0.6910        \u001b[31m0.7894\u001b[0m  0.0005  0.1681\n",
      "     33            1.0000        \u001b[32m0.0887\u001b[0m            0.6910        \u001b[31m0.7704\u001b[0m  0.0005  0.1665\n",
      "     34            1.0000        \u001b[32m0.0855\u001b[0m            0.6979        0.7725  0.0005  0.1652\n",
      "     35            1.0000        0.0880            0.7014        0.8253  0.0005  0.1689\n",
      "     36            1.0000        \u001b[32m0.0809\u001b[0m            0.7049        0.8495  0.0005  0.1665\n",
      "     37            1.0000        \u001b[32m0.0764\u001b[0m            \u001b[35m0.7222\u001b[0m        0.8058  0.0004  0.1656\n",
      "     38            1.0000        \u001b[32m0.0711\u001b[0m            0.7188        \u001b[31m0.7630\u001b[0m  0.0004  0.1664\n",
      "     39            1.0000        \u001b[32m0.0663\u001b[0m            0.7188        \u001b[31m0.7533\u001b[0m  0.0004  0.1654\n",
      "     40            1.0000        0.0682            0.7188        0.7799  0.0004  0.1657\n",
      "     41            1.0000        \u001b[32m0.0571\u001b[0m            0.7153        0.8090  0.0004  0.1673\n",
      "     42            1.0000        0.0614            0.7188        0.8050  0.0004  0.1693\n",
      "     43            1.0000        \u001b[32m0.0556\u001b[0m            0.7188        0.7816  0.0004  0.1677\n",
      "     44            1.0000        0.0681            0.7049        0.7914  0.0004  0.1683\n",
      "     45            1.0000        \u001b[32m0.0511\u001b[0m            0.7222        0.7837  0.0004  0.1685\n",
      "     46            1.0000        0.0579            \u001b[35m0.7326\u001b[0m        0.7729  0.0004  0.1669\n",
      "     47            1.0000        0.0599            0.7153        0.7626  0.0003  0.1688\n",
      "     48            1.0000        0.0513            0.7188        0.7837  0.0003  0.1690\n",
      "     49            1.0000        \u001b[32m0.0431\u001b[0m            0.7153        0.8024  0.0003  0.1678\n",
      "     50            1.0000        0.0536            0.7188        0.7931  0.0003  0.1665\n",
      "     51            1.0000        0.0503            0.7188        0.7787  0.0003  0.1711\n",
      "     52            1.0000        0.0521            0.7083        0.7913  0.0003  0.1678\n",
      "     53            1.0000        0.0438            0.7257        \u001b[31m0.7483\u001b[0m  0.0003  0.1678\n",
      "     54            1.0000        \u001b[32m0.0348\u001b[0m            0.7222        \u001b[31m0.7391\u001b[0m  0.0003  0.1677\n",
      "     55            1.0000        0.0506            0.7153        0.7660  0.0003  0.1684\n",
      "     56            1.0000        0.0399            0.7292        0.7769  0.0003  0.1685\n",
      "     57            1.0000        0.0387            0.7222        0.7822  0.0002  0.1741\n",
      "     58            1.0000        0.0369            0.7326        0.7800  0.0002  0.1688\n",
      "     59            1.0000        0.0377            \u001b[35m0.7361\u001b[0m        0.7844  0.0002  0.1695\n",
      "     60            1.0000        0.0434            0.7257        0.7769  0.0002  0.1689\n",
      "     61            1.0000        0.0360            0.7257        0.7697  0.0002  0.1666\n",
      "     62            1.0000        0.0371            0.7153        0.7704  0.0002  0.1694\n",
      "     63            1.0000        \u001b[32m0.0329\u001b[0m            0.7188        0.7641  0.0002  0.1695\n",
      "     64            1.0000        0.0379            0.7222        0.7470  0.0002  0.1684\n",
      "     65            1.0000        \u001b[32m0.0319\u001b[0m            0.7257        \u001b[31m0.7279\u001b[0m  0.0002  0.1689\n",
      "     66            1.0000        \u001b[32m0.0292\u001b[0m            0.7222        0.7313  0.0002  0.1670\n",
      "     67            1.0000        0.0355            0.7188        0.7480  0.0002  0.1683\n",
      "     68            1.0000        0.0297            0.7222        0.7452  0.0001  0.1693\n",
      "     69            1.0000        \u001b[32m0.0282\u001b[0m            0.7222        0.7438  0.0001  0.1706\n",
      "     70            1.0000        0.0429            0.7222        0.7587  0.0001  0.1694\n",
      "     71            1.0000        \u001b[32m0.0273\u001b[0m            0.7188        0.7671  0.0001  0.1684\n",
      "     72            1.0000        0.0325            0.7188        0.7681  0.0001  0.1707\n",
      "     73            1.0000        0.0287            0.7222        0.7627  0.0001  0.1682\n",
      "     74            1.0000        \u001b[32m0.0259\u001b[0m            0.7326        0.7516  0.0001  0.1659\n",
      "     75            1.0000        \u001b[32m0.0220\u001b[0m            0.7326        0.7440  0.0001  0.1677\n",
      "     76            1.0000        0.0411            \u001b[35m0.7396\u001b[0m        0.7409  0.0001  0.1659\n",
      "     77            1.0000        0.0289            \u001b[35m0.7431\u001b[0m        0.7394  0.0001  0.1667\n",
      "     78            1.0000        0.0298            0.7431        0.7436  0.0001  0.1668\n",
      "     79            1.0000        0.0280            0.7431        0.7492  0.0001  0.1673\n",
      "     80            1.0000        \u001b[32m0.0210\u001b[0m            0.7361        0.7587  0.0001  0.1660\n",
      "     81            1.0000        0.0291            0.7396        0.7578  0.0001  0.1699\n",
      "     82            1.0000        0.0263            0.7361        0.7597  0.0000  0.1695\n",
      "     83            1.0000        0.0279            0.7396        0.7607  0.0000  0.1672\n",
      "     84            1.0000        0.0290            0.7396        0.7544  0.0000  0.1698\n",
      "     85            1.0000        0.0285            0.7431        0.7534  0.0000  0.1695\n",
      "     86            1.0000        0.0249            \u001b[35m0.7465\u001b[0m        0.7512  0.0000  0.1711\n",
      "     87            1.0000        0.0414            0.7361        0.7570  0.0000  0.1691\n",
      "     88            1.0000        0.0227            0.7326        0.7576  0.0000  0.1742\n",
      "     89            1.0000        0.0232            0.7326        0.7564  0.0000  0.1668\n",
      "     90            1.0000        0.0353            0.7361        0.7519  0.0000  0.1697\n",
      "     91            1.0000        0.0247            0.7361        0.7494  0.0000  0.1684\n",
      "     92            1.0000        0.0253            0.7361        0.7489  0.0000  0.1695\n",
      "     93            1.0000        0.0258            0.7361        0.7481  0.0000  0.1680\n",
      "     94            1.0000        0.0258            0.7396        0.7439  0.0000  0.1682\n",
      "     95            1.0000        0.0283            0.7396        0.7432  0.0000  0.1671\n",
      "     96            1.0000        0.0321            0.7361        0.7451  0.0000  0.1682\n",
      "     97            1.0000        0.0325            0.7431        0.7399  0.0000  0.1675\n",
      "     98            1.0000        0.0268            0.7465        0.7386  0.0000  0.1674\n",
      "     99            1.0000        0.0263            0.7431        0.7365  0.0000  0.1680\n",
      "    100            1.0000        0.0284            0.7396        0.7429  0.0000  0.1651\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_channels, n_classes,\n",
    "    input_window_samples=n_samples,\n",
    "    final_conv_length='auto'\n",
    ")\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.NLLLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.0625 * 0.01\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        validX, validY))\n",
    "net = EEGClassifier(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            \"accuracy\",\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    trainX, y=trainY, epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training pipeline with ShallowFBCSPNet\n",
    "\n",
    "The difference with the above code is that we use `NeuralNetClassifierNoLog` by removing the log operation in `NeuralNetClassifier`, since ShallowFBCSPNet has already implemented `LogSoftmax` itself.\n",
    "\n",
    "our training pipeline can achieve similar training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1       \u001b[36m0.2917\u001b[0m        \u001b[32m1.5341\u001b[0m       \u001b[35m0.3194\u001b[0m        \u001b[31m1.7365\u001b[0m     +  0.0006  0.2156\n",
      "      2       \u001b[36m0.4340\u001b[0m        \u001b[32m1.2407\u001b[0m       \u001b[35m0.3611\u001b[0m        \u001b[31m1.6086\u001b[0m     +  0.0006  0.1708\n",
      "      3       \u001b[36m0.4931\u001b[0m        \u001b[32m1.0748\u001b[0m       \u001b[35m0.3958\u001b[0m        \u001b[31m1.4128\u001b[0m     +  0.0006  0.1671\n",
      "      4       \u001b[36m0.5729\u001b[0m        \u001b[32m0.9865\u001b[0m       \u001b[35m0.4444\u001b[0m        \u001b[31m1.2122\u001b[0m     +  0.0006  0.1675\n",
      "      5       \u001b[36m0.6424\u001b[0m        \u001b[32m0.8446\u001b[0m       \u001b[35m0.4861\u001b[0m        \u001b[31m1.1604\u001b[0m     +  0.0006  0.1665\n",
      "      6       \u001b[36m0.7083\u001b[0m        \u001b[32m0.7972\u001b[0m       \u001b[35m0.4965\u001b[0m        \u001b[31m1.1211\u001b[0m     +  0.0006  0.1682\n",
      "      7       \u001b[36m0.7431\u001b[0m        \u001b[32m0.6772\u001b[0m       \u001b[35m0.5347\u001b[0m        \u001b[31m1.0755\u001b[0m     +  0.0006  0.1670\n",
      "      8       \u001b[36m0.7674\u001b[0m        \u001b[32m0.6072\u001b[0m       \u001b[35m0.5417\u001b[0m        \u001b[31m1.0612\u001b[0m     +  0.0006  0.1675\n",
      "      9       \u001b[36m0.8229\u001b[0m        \u001b[32m0.5328\u001b[0m       \u001b[35m0.5486\u001b[0m        \u001b[31m1.0503\u001b[0m     +  0.0006  0.1685\n",
      "     10       0.8021        \u001b[32m0.5160\u001b[0m       \u001b[35m0.5660\u001b[0m        \u001b[31m1.0188\u001b[0m     +  0.0006  0.1661\n",
      "     11       \u001b[36m0.8646\u001b[0m        \u001b[32m0.4686\u001b[0m       \u001b[35m0.5903\u001b[0m        \u001b[31m0.9965\u001b[0m     +  0.0006  0.1679\n",
      "     12       \u001b[36m0.8785\u001b[0m        \u001b[32m0.3961\u001b[0m       \u001b[35m0.6389\u001b[0m        \u001b[31m0.9625\u001b[0m     +  0.0006  0.1687\n",
      "     13       \u001b[36m0.8854\u001b[0m        \u001b[32m0.3827\u001b[0m       0.6250        \u001b[31m0.9326\u001b[0m     +  0.0006  0.1666\n",
      "     14       \u001b[36m0.9028\u001b[0m        \u001b[32m0.3656\u001b[0m       0.6215        \u001b[31m0.9248\u001b[0m     +  0.0006  0.1681\n",
      "     15       \u001b[36m0.9167\u001b[0m        \u001b[32m0.2938\u001b[0m       0.6354        0.9344        0.0006  0.1678\n",
      "     16       0.9167        0.3004       0.6215        \u001b[31m0.9116\u001b[0m     +  0.0006  0.1667\n",
      "     17       \u001b[36m0.9271\u001b[0m        \u001b[32m0.2614\u001b[0m       0.6389        \u001b[31m0.8880\u001b[0m     +  0.0006  0.1674\n",
      "     18       0.9201        0.2654       0.6285        0.9098        0.0006  0.1687\n",
      "     19       \u001b[36m0.9583\u001b[0m        \u001b[32m0.2008\u001b[0m       \u001b[35m0.6493\u001b[0m        0.9354        0.0006  0.1664\n",
      "     20       \u001b[36m0.9653\u001b[0m        \u001b[32m0.1929\u001b[0m       0.6389        0.9304        0.0006  0.1670\n",
      "     21       0.9549        \u001b[32m0.1916\u001b[0m       0.6354        0.8921        0.0006  0.1691\n",
      "     22       \u001b[36m0.9757\u001b[0m        \u001b[32m0.1797\u001b[0m       \u001b[35m0.6528\u001b[0m        \u001b[31m0.8639\u001b[0m     +  0.0006  0.1661\n",
      "     23       0.9688        0.1880       \u001b[35m0.6632\u001b[0m        0.8826        0.0006  0.1665\n",
      "     24       0.9583        \u001b[32m0.1795\u001b[0m       0.6458        0.9457        0.0005  0.1678\n",
      "     25       0.9549        \u001b[32m0.1604\u001b[0m       0.6632        0.8658        0.0005  0.1679\n",
      "     26       \u001b[36m0.9896\u001b[0m        \u001b[32m0.1351\u001b[0m       \u001b[35m0.6736\u001b[0m        \u001b[31m0.8484\u001b[0m     +  0.0005  0.1665\n",
      "     27       0.9792        0.1365       \u001b[35m0.6806\u001b[0m        \u001b[31m0.8424\u001b[0m     +  0.0005  0.1673\n",
      "     28       0.9826        \u001b[32m0.1312\u001b[0m       \u001b[35m0.6910\u001b[0m        \u001b[31m0.8401\u001b[0m     +  0.0005  0.1672\n",
      "     29       0.9792        \u001b[32m0.1030\u001b[0m       \u001b[35m0.7083\u001b[0m        \u001b[31m0.8339\u001b[0m     +  0.0005  0.1685\n",
      "     30       0.9757        0.1249       0.6840        0.8404        0.0005  0.1672\n",
      "     31       0.9896        \u001b[32m0.0913\u001b[0m       0.6840        \u001b[31m0.8158\u001b[0m     +  0.0005  0.1663\n",
      "     32       0.9861        0.1066       0.6910        \u001b[31m0.7894\u001b[0m     +  0.0005  0.1678\n",
      "     33       0.9896        \u001b[32m0.0887\u001b[0m       0.6910        \u001b[31m0.7704\u001b[0m     +  0.0005  0.1658\n",
      "     34       0.9896        \u001b[32m0.0855\u001b[0m       0.6979        0.7725        0.0005  0.1659\n",
      "     35       0.9757        0.0880       0.7014        0.8253        0.0005  0.1678\n",
      "     36       \u001b[36m0.9965\u001b[0m        \u001b[32m0.0809\u001b[0m       0.7049        0.8495        0.0005  0.1653\n",
      "     37       0.9965        \u001b[32m0.0764\u001b[0m       \u001b[35m0.7222\u001b[0m        0.8058        0.0004  0.1657\n",
      "     38       0.9931        \u001b[32m0.0711\u001b[0m       0.7188        \u001b[31m0.7630\u001b[0m     +  0.0004  0.1671\n",
      "     39       \u001b[36m1.0000\u001b[0m        \u001b[32m0.0663\u001b[0m       0.7188        \u001b[31m0.7533\u001b[0m     +  0.0004  0.1664\n",
      "     40       0.9896        0.0682       0.7188        0.7799        0.0004  0.1657\n",
      "     41       1.0000        \u001b[32m0.0571\u001b[0m       0.7153        0.8090        0.0004  0.1667\n",
      "     42       0.9896        0.0614       0.7188        0.8050        0.0004  0.1669\n",
      "     43       0.9931        \u001b[32m0.0556\u001b[0m       0.7188        0.7816        0.0004  0.1657\n",
      "     44       0.9931        0.0681       0.7049        0.7914        0.0004  0.1668\n",
      "     45       0.9896        \u001b[32m0.0511\u001b[0m       0.7222        0.7837        0.0004  0.1654\n",
      "     46       0.9931        0.0579       \u001b[35m0.7326\u001b[0m        0.7729        0.0004  0.1666\n",
      "     47       0.9931        0.0599       0.7153        0.7626        0.0003  0.1670\n",
      "     48       0.9965        0.0513       0.7188        0.7837        0.0003  0.1658\n",
      "     49       0.9965        \u001b[32m0.0431\u001b[0m       0.7153        0.8024        0.0003  0.1664\n",
      "     50       0.9896        0.0536       0.7188        0.7932        0.0003  0.1664\n",
      "     51       0.9965        0.0503       0.7188        0.7787        0.0003  0.1655\n",
      "     52       0.9965        0.0521       0.7083        0.7913        0.0003  0.1658\n",
      "     53       1.0000        0.0438       0.7257        \u001b[31m0.7483\u001b[0m     +  0.0003  0.1676\n",
      "     54       1.0000        \u001b[32m0.0348\u001b[0m       0.7222        \u001b[31m0.7391\u001b[0m     +  0.0003  0.1660\n",
      "     55       0.9861        0.0506       0.7153        0.7660        0.0003  0.1658\n",
      "     56       0.9931        0.0399       0.7292        0.7769        0.0003  0.1680\n",
      "     57       1.0000        0.0387       0.7222        0.7822        0.0002  0.1659\n",
      "     58       1.0000        0.0369       0.7326        0.7800        0.0002  0.1669\n",
      "     59       0.9965        0.0377       \u001b[35m0.7361\u001b[0m        0.7844        0.0002  0.1678\n",
      "     60       0.9965        0.0434       0.7257        0.7769        0.0002  0.1667\n",
      "     61       0.9965        0.0360       0.7257        0.7697        0.0002  0.1668\n",
      "     62       1.0000        0.0371       0.7153        0.7704        0.0002  0.1661\n",
      "     63       1.0000        \u001b[32m0.0329\u001b[0m       0.7188        0.7641        0.0002  0.1677\n",
      "     64       0.9965        0.0379       0.7222        0.7470        0.0002  0.1675\n",
      "     65       0.9965        \u001b[32m0.0319\u001b[0m       0.7257        \u001b[31m0.7279\u001b[0m     +  0.0002  0.1665\n",
      "     66       0.9965        \u001b[32m0.0292\u001b[0m       0.7222        0.7313        0.0002  0.1657\n",
      "     67       1.0000        0.0355       0.7188        0.7480        0.0002  0.1674\n",
      "     68       1.0000        0.0297       0.7222        0.7452        0.0001  0.1656\n",
      "     69       1.0000        \u001b[32m0.0282\u001b[0m       0.7222        0.7438        0.0001  0.1656\n",
      "     70       0.9931        0.0429       0.7222        0.7587        0.0001  0.1678\n",
      "     71       1.0000        \u001b[32m0.0273\u001b[0m       0.7188        0.7671        0.0001  0.1663\n",
      "     72       1.0000        0.0325       0.7188        0.7681        0.0001  0.1663\n",
      "     73       1.0000        0.0287       0.7222        0.7627        0.0001  0.1671\n",
      "     74       1.0000        \u001b[32m0.0259\u001b[0m       0.7326        0.7516        0.0001  0.1670\n",
      "     75       1.0000        \u001b[32m0.0220\u001b[0m       0.7326        0.7440        0.0001  0.1663\n",
      "     76       0.9896        0.0411       \u001b[35m0.7396\u001b[0m        0.7409        0.0001  0.1670\n",
      "     77       1.0000        0.0289       \u001b[35m0.7431\u001b[0m        0.7394        0.0001  0.1671\n",
      "     78       0.9965        0.0298       0.7431        0.7436        0.0001  0.1656\n",
      "     79       0.9965        0.0280       0.7431        0.7492        0.0001  0.1667\n",
      "     80       1.0000        \u001b[32m0.0210\u001b[0m       0.7361        0.7587        0.0001  0.1665\n",
      "     81       0.9965        0.0291       0.7396        0.7578        0.0001  0.1670\n",
      "     82       0.9965        0.0263       0.7361        0.7597        0.0000  0.1672\n",
      "     83       1.0000        0.0279       0.7396        0.7607        0.0000  0.1695\n",
      "     84       1.0000        0.0290       0.7396        0.7544        0.0000  0.1672\n",
      "     85       1.0000        0.0285       0.7431        0.7534        0.0000  0.1676\n",
      "     86       0.9965        0.0249       \u001b[35m0.7465\u001b[0m        0.7512        0.0000  0.1665\n",
      "     87       0.9896        0.0414       0.7361        0.7570        0.0000  0.1665\n",
      "     88       1.0000        0.0227       0.7326        0.7576        0.0000  0.1677\n",
      "     89       1.0000        0.0232       0.7326        0.7564        0.0000  0.1669\n",
      "     90       1.0000        0.0353       0.7361        0.7519        0.0000  0.1673\n",
      "     91       1.0000        0.0247       0.7361        0.7494        0.0000  0.1676\n",
      "     92       0.9965        0.0253       0.7361        0.7489        0.0000  0.1659\n",
      "     93       0.9965        0.0258       0.7361        0.7481        0.0000  0.1669\n",
      "     94       0.9965        0.0258       0.7396        0.7439        0.0000  0.1680\n",
      "     95       1.0000        0.0283       0.7396        0.7432        0.0000  0.1666\n",
      "     96       0.9896        0.0321       0.7361        0.7451        0.0000  0.1668\n",
      "     97       1.0000        0.0325       0.7431        0.7399        0.0000  0.1665\n",
      "     98       1.0000        0.0268       0.7465        0.7386        0.0000  0.1674\n",
      "     99       1.0000        0.0263       0.7431        0.7365        0.0000  0.1672\n",
      "    100       0.9965        0.0284       0.7396        0.7429        0.0000  0.1664\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetClassifierNoLog(NeuralNetClassifier):\n",
    "    def get_loss(self, y_pred, y_true, *args, **kwargs):\n",
    "        return super(NeuralNetClassifier, self).get_loss(y_pred, y_true, *args, **kwargs)\n",
    "\n",
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = ShallowFBCSPNet(\n",
    "    n_channels, n_classes,\n",
    "    input_window_samples=n_samples,\n",
    "    final_conv_length='auto'\n",
    ")\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.NLLLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.0625 * 0.01\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ckp = Checkpoint(dirname=ckp_dirname)\n",
    "train_end_ckp = TrainEndCheckpoint(dirname=ckp_dirname)\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        validX, validY))\n",
    "net = NeuralNetClassifierNoLog(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            ('train_acc', EpochScoring('accuracy', \n",
    "                                    name='train_acc', \n",
    "                                    on_train=True, \n",
    "                                    lower_is_better=False)),\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "            ckp,\n",
    "            train_end_ckp\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    trainX, y=trainY, epochs=max_epochs)\n",
    "net.load_params(checkpoint=ckp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training pipeline with ShallowNet\n",
    "\n",
    "Here we use the same training pipeline with self-implemented ShallowNet modified from ShallowFBCSPNet, replacing `conv_classifier` with `fc_layer` and removing `LogSoftmax`.\n",
    "\n",
    "Similar results with small improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1       \u001b[36m0.2222\u001b[0m        \u001b[32m1.6694\u001b[0m       \u001b[35m0.2951\u001b[0m        \u001b[31m1.7453\u001b[0m     +  0.0006  0.6310\n",
      "      2       \u001b[36m0.4306\u001b[0m        \u001b[32m1.2924\u001b[0m       \u001b[35m0.3403\u001b[0m        \u001b[31m1.5502\u001b[0m     +  0.0006  0.2201\n",
      "      3       \u001b[36m0.5139\u001b[0m        \u001b[32m1.1181\u001b[0m       \u001b[35m0.3819\u001b[0m        \u001b[31m1.4406\u001b[0m     +  0.0006  0.2139\n",
      "      4       \u001b[36m0.5486\u001b[0m        \u001b[32m1.0909\u001b[0m       \u001b[35m0.4549\u001b[0m        \u001b[31m1.3102\u001b[0m     +  0.0006  0.2147\n",
      "      5       \u001b[36m0.6042\u001b[0m        \u001b[32m0.9650\u001b[0m       \u001b[35m0.4653\u001b[0m        \u001b[31m1.1612\u001b[0m     +  0.0006  0.2130\n",
      "      6       \u001b[36m0.6632\u001b[0m        \u001b[32m0.8896\u001b[0m       \u001b[35m0.5243\u001b[0m        \u001b[31m1.1059\u001b[0m     +  0.0006  0.2147\n",
      "      7       \u001b[36m0.7326\u001b[0m        \u001b[32m0.7701\u001b[0m       \u001b[35m0.5312\u001b[0m        \u001b[31m1.0827\u001b[0m     +  0.0006  0.2155\n",
      "      8       \u001b[36m0.7569\u001b[0m        \u001b[32m0.7109\u001b[0m       \u001b[35m0.5451\u001b[0m        \u001b[31m1.0511\u001b[0m     +  0.0006  0.2136\n",
      "      9       \u001b[36m0.7847\u001b[0m        \u001b[32m0.6344\u001b[0m       \u001b[35m0.5556\u001b[0m        1.0688        0.0006  0.2134\n",
      "     10       \u001b[36m0.8229\u001b[0m        \u001b[32m0.5440\u001b[0m       \u001b[35m0.5625\u001b[0m        1.0805        0.0006  0.2150\n",
      "     11       \u001b[36m0.8264\u001b[0m        \u001b[32m0.5083\u001b[0m       \u001b[35m0.6042\u001b[0m        \u001b[31m1.0113\u001b[0m     +  0.0006  0.2147\n",
      "     12       \u001b[36m0.8785\u001b[0m        \u001b[32m0.4223\u001b[0m       \u001b[35m0.6215\u001b[0m        \u001b[31m0.9217\u001b[0m     +  0.0006  0.2134\n",
      "     13       0.8750        \u001b[32m0.4102\u001b[0m       \u001b[35m0.6354\u001b[0m        \u001b[31m0.8961\u001b[0m     +  0.0006  0.2154\n",
      "     14       0.8750        \u001b[32m0.3960\u001b[0m       \u001b[35m0.6493\u001b[0m        \u001b[31m0.8671\u001b[0m     +  0.0006  0.2135\n",
      "     15       \u001b[36m0.9201\u001b[0m        \u001b[32m0.3074\u001b[0m       \u001b[35m0.6736\u001b[0m        \u001b[31m0.8331\u001b[0m     +  0.0006  0.2144\n",
      "     16       0.9167        \u001b[32m0.2925\u001b[0m       0.6736        0.8350        0.0006  0.2124\n",
      "     17       \u001b[36m0.9514\u001b[0m        \u001b[32m0.2600\u001b[0m       \u001b[35m0.6840\u001b[0m        \u001b[31m0.8142\u001b[0m     +  0.0006  0.2124\n",
      "     18       0.9236        0.2698       \u001b[35m0.7049\u001b[0m        \u001b[31m0.7770\u001b[0m     +  0.0006  0.2147\n",
      "     19       0.9306        0.2641       0.6979        \u001b[31m0.7566\u001b[0m     +  0.0006  0.2138\n",
      "     20       0.9444        \u001b[32m0.2138\u001b[0m       0.6806        0.7668        0.0006  0.2139\n",
      "     21       0.9410        0.2145       0.7014        \u001b[31m0.7494\u001b[0m     +  0.0006  0.2136\n",
      "     22       0.9340        0.2265       \u001b[35m0.7153\u001b[0m        \u001b[31m0.7418\u001b[0m     +  0.0006  0.2151\n",
      "     23       \u001b[36m0.9826\u001b[0m        \u001b[32m0.1567\u001b[0m       0.6806        0.7763        0.0006  0.2139\n",
      "     24       0.9514        0.1843       0.6806        0.8052        0.0005  0.2148\n",
      "     25       0.9757        0.1568       \u001b[35m0.7292\u001b[0m        \u001b[31m0.7408\u001b[0m     +  0.0005  0.2132\n",
      "     26       0.9688        \u001b[32m0.1556\u001b[0m       \u001b[35m0.7465\u001b[0m        \u001b[31m0.6948\u001b[0m     +  0.0005  0.2141\n",
      "     27       0.9792        \u001b[32m0.1342\u001b[0m       0.7292        0.7166        0.0005  0.2141\n",
      "     28       0.9826        \u001b[32m0.0979\u001b[0m       0.7396        0.7209        0.0005  0.2130\n",
      "     29       0.9757        0.1275       0.7326        0.7779        0.0005  0.2179\n",
      "     30       0.9722        0.1299       0.7153        0.7863        0.0005  0.2121\n",
      "     31       \u001b[36m0.9861\u001b[0m        0.1037       0.7361        0.7307        0.0005  0.2148\n",
      "     32       0.9861        0.0980       0.7431        0.7211        0.0005  0.2133\n",
      "     33       \u001b[36m0.9965\u001b[0m        \u001b[32m0.0744\u001b[0m       0.7465        0.7118        0.0005  0.2132\n",
      "     34       0.9792        0.0920       0.7396        \u001b[31m0.6734\u001b[0m     +  0.0005  0.2147\n",
      "     35       0.9965        0.0755       0.7465        \u001b[31m0.6699\u001b[0m     +  0.0005  0.2121\n",
      "     36       0.9965        \u001b[32m0.0638\u001b[0m       \u001b[35m0.7569\u001b[0m        0.6767        0.0005  0.2141\n",
      "     37       0.9861        0.0828       0.7292        0.7131        0.0004  0.2145\n",
      "     38       0.9896        0.0723       0.7188        0.7319        0.0004  0.2141\n",
      "     39       0.9931        0.0668       0.7326        0.6709        0.0004  0.2135\n",
      "     40       0.9965        \u001b[32m0.0600\u001b[0m       0.7535        \u001b[31m0.6286\u001b[0m     +  0.0004  0.2156\n",
      "     41       0.9931        \u001b[32m0.0574\u001b[0m       0.7535        \u001b[31m0.6262\u001b[0m     +  0.0004  0.2125\n",
      "     42       0.9931        0.0714       \u001b[35m0.7604\u001b[0m        \u001b[31m0.6254\u001b[0m     +  0.0004  0.2126\n",
      "     43       0.9965        0.0608       \u001b[35m0.7708\u001b[0m        0.6361        0.0004  0.2157\n",
      "     44       0.9826        0.0655       0.7639        0.6317        0.0004  0.2127\n",
      "     45       0.9931        \u001b[32m0.0573\u001b[0m       0.7535        0.6728        0.0004  0.2141\n",
      "     46       0.9931        0.0657       0.7431        0.6440        0.0004  0.2132\n",
      "     47       \u001b[36m1.0000\u001b[0m        \u001b[32m0.0407\u001b[0m       0.7535        \u001b[31m0.6149\u001b[0m     +  0.0003  0.2144\n",
      "     48       0.9965        0.0426       0.7569        0.6235        0.0003  0.2145\n",
      "     49       0.9896        0.0649       0.7639        0.6245        0.0003  0.2136\n",
      "     50       0.9965        0.0519       0.7708        \u001b[31m0.5912\u001b[0m     +  0.0003  0.2120\n",
      "     51       0.9965        0.0446       0.7639        0.6016        0.0003  0.2156\n",
      "     52       0.9965        \u001b[32m0.0345\u001b[0m       0.7569        0.6193        0.0003  0.2144\n",
      "     53       1.0000        \u001b[32m0.0341\u001b[0m       0.7535        0.6265        0.0003  0.2133\n",
      "     54       0.9965        0.0525       0.7535        0.6152        0.0003  0.2157\n",
      "     55       0.9965        0.0426       0.7569        0.6373        0.0003  0.2130\n",
      "     56       0.9965        0.0400       0.7604        0.6455        0.0003  0.2145\n",
      "     57       1.0000        0.0352       0.7639        0.6351        0.0002  0.2133\n",
      "     58       1.0000        0.0363       0.7674        0.6109        0.0002  0.2132\n",
      "     59       0.9965        0.0422       0.7708        0.5961        0.0002  0.2152\n",
      "     60       1.0000        \u001b[32m0.0287\u001b[0m       0.7569        0.6079        0.0002  0.2126\n",
      "     61       0.9965        0.0316       0.7535        0.6311        0.0002  0.2151\n",
      "     62       1.0000        0.0292       0.7500        0.6501        0.0002  0.2140\n",
      "     63       0.9965        0.0390       0.7604        0.6468        0.0002  0.2153\n",
      "     64       0.9965        0.0362       0.7569        0.6477        0.0002  0.2120\n",
      "     65       0.9965        0.0294       0.7639        0.6303        0.0002  0.2145\n",
      "     66       1.0000        \u001b[32m0.0271\u001b[0m       0.7639        0.6294        0.0002  0.2129\n",
      "     67       0.9965        0.0300       0.7604        0.6105        0.0002  0.2132\n",
      "     68       0.9965        0.0332       0.7639        0.5916        0.0001  0.2138\n",
      "     69       0.9965        0.0340       0.7639        \u001b[31m0.5694\u001b[0m     +  0.0001  0.2135\n",
      "     70       1.0000        \u001b[32m0.0256\u001b[0m       0.7639        \u001b[31m0.5665\u001b[0m     +  0.0001  0.2144\n",
      "     71       1.0000        0.0355       0.7604        0.5770        0.0001  0.2124\n",
      "     72       1.0000        0.0269       0.7604        0.5830        0.0001  0.2131\n",
      "     73       1.0000        \u001b[32m0.0230\u001b[0m       0.7708        0.5959        0.0001  0.2138\n",
      "     74       1.0000        0.0308       \u001b[35m0.7743\u001b[0m        0.6009        0.0001  0.2128\n",
      "     75       1.0000        0.0309       0.7708        0.6063        0.0001  0.2141\n",
      "     76       1.0000        0.0275       0.7674        0.6079        0.0001  0.2126\n",
      "     77       0.9931        0.0302       0.7708        0.6204        0.0001  0.2142\n",
      "     78       1.0000        \u001b[32m0.0216\u001b[0m       0.7743        0.6110        0.0001  0.2136\n",
      "     79       0.9931        0.0308       0.7708        0.6104        0.0001  0.2150\n",
      "     80       1.0000        0.0226       0.7674        0.6041        0.0001  0.2126\n",
      "     81       0.9965        0.0260       0.7674        0.6029        0.0001  0.2139\n",
      "     82       0.9965        0.0320       0.7674        0.5991        0.0000  0.2126\n",
      "     83       0.9965        0.0261       0.7674        0.5982        0.0000  0.2125\n",
      "     84       0.9965        0.0241       0.7708        0.6006        0.0000  0.2150\n",
      "     85       0.9965        0.0276       0.7639        0.6078        0.0000  0.2131\n",
      "     86       1.0000        0.0305       0.7639        0.6095        0.0000  0.2131\n",
      "     87       0.9965        0.0333       0.7674        0.6093        0.0000  0.2130\n",
      "     88       0.9965        0.0303       0.7674        0.6013        0.0000  0.2131\n",
      "     89       0.9965        0.0311       0.7708        0.5989        0.0000  0.2135\n",
      "     90       1.0000        0.0271       0.7674        0.6014        0.0000  0.2131\n",
      "     91       1.0000        0.0278       0.7674        0.6059        0.0000  0.2138\n",
      "     92       1.0000        0.0300       0.7674        0.6012        0.0000  0.2138\n",
      "     93       1.0000        0.0273       0.7674        0.6033        0.0000  0.2139\n",
      "     94       0.9965        0.0373       0.7674        0.5997        0.0000  0.2123\n",
      "     95       1.0000        0.0257       0.7639        0.6042        0.0000  0.2153\n",
      "     96       1.0000        0.0218       0.7639        0.6011        0.0000  0.2132\n",
      "     97       1.0000        0.0237       0.7674        0.6014        0.0000  0.2128\n",
      "     98       1.0000        0.0302       0.7674        0.6088        0.0000  0.2149\n",
      "     99       0.9965        0.0311       0.7639        0.6022        0.0000  0.2125\n",
      "    100       1.0000        0.0268       0.7639        0.6023        0.0000  0.2155\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = ShallowNet(\n",
    "    n_channels, n_samples, n_classes)\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.CrossEntropyLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.0625 * 0.01\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ckp = Checkpoint(dirname=ckp_dirname)\n",
    "train_end_ckp = TrainEndCheckpoint(dirname=ckp_dirname)\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        {'X': validX}, validY))\n",
    "net = NeuralNetClassifierNoLog(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            ('train_acc', EpochScoring('accuracy', \n",
    "                                    name='train_acc', \n",
    "                                    on_train=True, \n",
    "                                    lower_is_better=False)),\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "            ckp,\n",
    "            train_end_ckp\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    {'X': trainX}, y=trainY, epochs=max_epochs)\n",
    "net.load_params(checkpoint=ckp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training pipeline with EEGNetv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1       \u001b[36m0.2917\u001b[0m        \u001b[32m1.5321\u001b[0m       \u001b[35m0.3681\u001b[0m        \u001b[31m1.3416\u001b[0m     +  0.0100  0.3494\n",
      "      2       \u001b[36m0.4132\u001b[0m        \u001b[32m1.2833\u001b[0m       \u001b[35m0.3889\u001b[0m        1.3704        0.0100  0.2431\n",
      "      3       \u001b[36m0.4757\u001b[0m        \u001b[32m1.1720\u001b[0m       \u001b[35m0.4861\u001b[0m        1.3713        0.0100  0.2401\n",
      "      4       \u001b[36m0.5486\u001b[0m        \u001b[32m1.0289\u001b[0m       0.4583        1.5700        0.0100  0.2401\n",
      "      5       \u001b[36m0.6319\u001b[0m        \u001b[32m0.8532\u001b[0m       0.4132        1.8973        0.0100  0.2407\n",
      "      6       \u001b[36m0.6701\u001b[0m        \u001b[32m0.8333\u001b[0m       0.4167        2.0515        0.0099  0.2400\n",
      "      7       \u001b[36m0.7188\u001b[0m        \u001b[32m0.7220\u001b[0m       0.4583        1.7437        0.0099  0.2409\n",
      "      8       \u001b[36m0.7639\u001b[0m        \u001b[32m0.6607\u001b[0m       \u001b[35m0.4965\u001b[0m        1.4679        0.0099  0.2392\n",
      "      9       \u001b[36m0.7778\u001b[0m        \u001b[32m0.5817\u001b[0m       \u001b[35m0.5903\u001b[0m        \u001b[31m1.2075\u001b[0m     +  0.0098  0.2407\n",
      "     10       0.7500        0.6228       \u001b[35m0.6181\u001b[0m        \u001b[31m0.9796\u001b[0m     +  0.0098  0.2389\n",
      "     11       \u001b[36m0.8125\u001b[0m        \u001b[32m0.4854\u001b[0m       \u001b[35m0.6667\u001b[0m        1.0270        0.0098  0.2404\n",
      "     12       \u001b[36m0.8194\u001b[0m        \u001b[32m0.4302\u001b[0m       \u001b[35m0.6944\u001b[0m        1.0325        0.0097  0.2395\n",
      "     13       \u001b[36m0.8646\u001b[0m        \u001b[32m0.3580\u001b[0m       0.6528        1.1578        0.0096  0.2397\n",
      "     14       0.8056        0.4836       0.6354        1.3109        0.0096  0.2406\n",
      "     15       0.8576        0.3824       0.6597        1.1630        0.0095  0.2405\n",
      "     16       \u001b[36m0.8715\u001b[0m        \u001b[32m0.3212\u001b[0m       \u001b[35m0.7083\u001b[0m        1.0989        0.0094  0.2405\n",
      "     17       0.8472        0.3925       \u001b[35m0.7257\u001b[0m        1.0147        0.0094  0.2412\n",
      "     18       \u001b[36m0.8750\u001b[0m        0.3879       0.6910        1.1617        0.0093  0.2391\n",
      "     19       0.8750        \u001b[32m0.3115\u001b[0m       \u001b[35m0.7361\u001b[0m        0.9891        0.0092  0.2414\n",
      "     20       \u001b[36m0.8819\u001b[0m        \u001b[32m0.2800\u001b[0m       0.6493        1.3175        0.0091  0.2391\n",
      "     21       0.8750        0.2882       \u001b[35m0.7500\u001b[0m        0.9841        0.0090  0.2421\n",
      "     22       \u001b[36m0.9097\u001b[0m        \u001b[32m0.2258\u001b[0m       \u001b[35m0.7639\u001b[0m        \u001b[31m0.8613\u001b[0m     +  0.0089  0.2394\n",
      "     23       \u001b[36m0.9167\u001b[0m        0.2387       0.6944        1.1851        0.0088  0.2396\n",
      "     24       \u001b[36m0.9271\u001b[0m        \u001b[32m0.1689\u001b[0m       \u001b[35m0.7847\u001b[0m        0.9564        0.0087  0.2423\n",
      "     25       \u001b[36m0.9410\u001b[0m        0.1984       0.7500        0.9639        0.0086  0.2401\n",
      "     26       0.9340        0.1836       \u001b[35m0.8333\u001b[0m        \u001b[31m0.8233\u001b[0m     +  0.0085  0.2405\n",
      "     27       0.9340        0.2217       0.7361        1.0994        0.0084  0.2403\n",
      "     28       0.8924        0.2407       0.6806        1.5060        0.0083  0.2400\n",
      "     29       0.9306        \u001b[32m0.1678\u001b[0m       0.6285        1.6111        0.0082  0.2403\n",
      "     30       0.9306        0.2198       0.7396        1.3247        0.0080  0.2405\n",
      "     31       0.9340        0.1797       0.4236        4.1633        0.0079  0.2406\n",
      "     32       0.9340        0.1956       0.4965        3.4878        0.0078  0.2443\n",
      "     33       \u001b[36m0.9444\u001b[0m        0.1758       0.6493        1.8716        0.0076  0.2395\n",
      "     34       0.9444        \u001b[32m0.1415\u001b[0m       0.8333        0.9353        0.0075  0.2400\n",
      "     35       0.9375        0.1718       \u001b[35m0.8576\u001b[0m        \u001b[31m0.7634\u001b[0m     +  0.0074  0.2388\n",
      "     36       \u001b[36m0.9479\u001b[0m        0.1535       0.8125        0.7854        0.0072  0.2408\n",
      "     37       \u001b[36m0.9618\u001b[0m        \u001b[32m0.1202\u001b[0m       \u001b[35m0.8681\u001b[0m        \u001b[31m0.7052\u001b[0m     +  0.0071  0.2394\n",
      "     38       0.9514        \u001b[32m0.1158\u001b[0m       0.8299        0.8387        0.0069  0.2410\n",
      "     39       0.9444        0.1493       0.8264        \u001b[31m0.6501\u001b[0m     +  0.0068  0.2404\n",
      "     40       0.9479        0.1201       0.7604        0.9376        0.0066  0.2393\n",
      "     41       0.9375        0.1711       0.7986        0.7663        0.0065  0.2399\n",
      "     42       0.9549        0.1197       0.8438        0.7259        0.0063  0.2402\n",
      "     43       0.9618        0.1223       0.7465        1.3937        0.0062  0.2401\n",
      "     44       0.9479        0.1410       0.8368        0.8187        0.0060  0.2407\n",
      "     45       0.9514        0.1260       0.8646        0.7066        0.0059  0.2390\n",
      "     46       \u001b[36m0.9722\u001b[0m        \u001b[32m0.0921\u001b[0m       0.7604        1.2757        0.0057  0.2422\n",
      "     47       0.9653        0.1122       0.7083        1.7116        0.0056  0.2388\n",
      "     48       0.9653        0.1013       0.8542        0.6878        0.0054  0.2398\n",
      "     49       0.9444        0.1363       0.8438        0.7255        0.0052  0.2362\n",
      "     50       0.9549        0.1447       0.8194        0.8515        0.0051  0.2379\n",
      "     51       0.9688        0.1030       0.8264        0.6605        0.0049  0.2374\n",
      "     52       0.9653        0.1016       0.8333        0.7452        0.0048  0.2372\n",
      "     53       0.9583        0.0935       0.8403        0.7264        0.0046  0.2375\n",
      "     54       0.9306        0.1500       0.7812        0.7955        0.0044  0.2373\n",
      "     55       0.9653        \u001b[32m0.0863\u001b[0m       0.8542        \u001b[31m0.6058\u001b[0m     +  0.0043  0.2365\n",
      "     56       \u001b[36m0.9757\u001b[0m        0.0984       0.7882        1.0584        0.0041  0.2389\n",
      "     57       0.9514        0.1153       0.8229        0.8439        0.0040  0.2368\n",
      "     58       \u001b[36m0.9826\u001b[0m        \u001b[32m0.0552\u001b[0m       0.8229        0.8068        0.0038  0.2389\n",
      "     59       0.9722        0.0659       0.7778        0.8334        0.0037  0.2364\n",
      "     60       0.9688        0.1064       0.8438        0.6835        0.0035  0.2373\n",
      "     61       0.9549        0.0914       0.8368        0.7356        0.0034  0.2379\n",
      "     62       0.9583        0.0902       0.8194        0.8297        0.0032  0.2374\n",
      "     63       0.9653        0.0792       0.8542        0.6690        0.0031  0.2414\n",
      "     64       0.9792        \u001b[32m0.0514\u001b[0m       0.8090        0.7788        0.0029  0.2389\n",
      "     65       0.9722        0.1046       0.7812        0.8639        0.0028  0.2389\n",
      "     66       0.9792        0.0677       0.7882        0.8819        0.0026  0.2400\n",
      "     67       \u001b[36m0.9896\u001b[0m        \u001b[32m0.0468\u001b[0m       0.8507        0.7520        0.0025  0.2399\n",
      "     68       0.9653        0.0754       0.8611        0.7258        0.0024  0.2400\n",
      "     69       0.9792        0.0522       0.8611        0.7251        0.0022  0.2406\n",
      "     70       0.9653        0.0866       0.8229        0.8090        0.0021  0.2389\n",
      "     71       0.9757        0.0811       0.8194        0.8598        0.0020  0.2406\n",
      "     72       0.9618        0.0768       0.8368        0.7905        0.0018  0.2387\n",
      "     73       0.9757        0.0636       0.8264        0.7190        0.0017  0.2402\n",
      "     74       0.9722        0.0795       0.8507        0.6912        0.0016  0.2386\n",
      "     75       0.9757        0.0731       0.8611        0.6955        0.0015  0.2398\n",
      "     76       0.9688        0.0777       0.8576        0.7142        0.0014  0.2403\n",
      "     77       0.9722        0.0642       0.8438        0.7218        0.0013  0.2402\n",
      "     78       0.9792        0.0516       0.8542        0.6951        0.0012  0.2407\n",
      "     79       0.9792        0.0578       0.8438        0.6887        0.0011  0.2399\n",
      "     80       0.9757        0.0587       0.8299        0.7216        0.0010  0.2403\n",
      "     81       0.9861        0.0576       0.8368        0.7094        0.0009  0.2404\n",
      "     82       0.9688        0.0789       0.8333        0.6927        0.0008  0.2391\n",
      "     83       0.9757        0.0729       0.8229        0.7007        0.0007  0.2413\n",
      "     84       0.9861        \u001b[32m0.0386\u001b[0m       0.8229        0.7300        0.0006  0.2393\n",
      "     85       0.9653        0.0979       0.8264        0.7468        0.0006  0.2404\n",
      "     86       0.9861        0.0525       0.8299        0.7347        0.0005  0.2390\n",
      "     87       0.9722        0.0623       0.8438        0.6935        0.0004  0.2398\n",
      "     88       0.9826        0.0405       0.8576        0.6662        0.0004  0.2405\n",
      "     89       0.9722        0.0575       0.8646        0.6523        0.0003  0.2403\n",
      "     90       0.9722        0.0783       0.8681        0.6484        0.0002  0.2404\n",
      "     91       0.9826        0.0650       0.8646        0.6422        0.0002  0.2401\n",
      "     92       0.9722        0.0938       0.8646        0.6389        0.0002  0.2389\n",
      "     93       0.9688        0.0586       0.8646        0.6391        0.0001  0.2417\n",
      "     94       0.9792        0.0841       0.8646        0.6410        0.0001  0.2383\n",
      "     95       0.9653        0.0896       0.8611        0.6410        0.0001  0.2413\n",
      "     96       0.9861        0.0609       0.8611        0.6413        0.0000  0.2394\n",
      "     97       0.9861        \u001b[32m0.0350\u001b[0m       0.8611        0.6425        0.0000  0.2403\n",
      "     98       0.9826        0.0529       0.8646        0.6438        0.0000  0.2394\n",
      "     99       \u001b[36m0.9931\u001b[0m        \u001b[32m0.0316\u001b[0m       0.8646        0.6452        0.0000  0.2393\n",
      "    100       0.9826        0.0514       0.8646        0.6463        0.0000  0.2401\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetClassifierNoLog(NeuralNetClassifier):\n",
    "    def get_loss(self, y_pred, y_true, *args, **kwargs):\n",
    "        return super(NeuralNetClassifier, self).get_loss(y_pred, y_true, *args, **kwargs)\n",
    "\n",
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = EEGNetv4(n_channels, n_classes, \n",
    "            input_window_samples=n_samples, \n",
    "            F1=8,\n",
    "            D=2,\n",
    "            F2=16,\n",
    "            kernel_length=64,\n",
    "            drop_prob=0.5)\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.NLLLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ckp = Checkpoint(dirname=ckp_dirname)\n",
    "train_end_ckp = TrainEndCheckpoint(dirname=ckp_dirname)\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        validX, validY))\n",
    "net = NeuralNetClassifierNoLog(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            ('train_acc', EpochScoring('accuracy', \n",
    "                                    name='train_acc', \n",
    "                                    on_train=True, \n",
    "                                    lower_is_better=False)),\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "            ckp,\n",
    "            train_end_ckp\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    trainX, y=trainY, epochs=max_epochs)\n",
    "net.load_params(checkpoint=ckp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training pipeline with EEGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1       \u001b[36m0.2396\u001b[0m        \u001b[32m1.5590\u001b[0m       \u001b[35m0.3333\u001b[0m        \u001b[31m1.3508\u001b[0m     +  0.0100  0.1676\n",
      "      2       \u001b[36m0.3299\u001b[0m        \u001b[32m1.4080\u001b[0m       \u001b[35m0.3576\u001b[0m        \u001b[31m1.3181\u001b[0m     +  0.0100  0.1197\n",
      "      3       \u001b[36m0.4167\u001b[0m        \u001b[32m1.2456\u001b[0m       \u001b[35m0.4097\u001b[0m        \u001b[31m1.2868\u001b[0m     +  0.0100  0.1186\n",
      "      4       \u001b[36m0.4479\u001b[0m        \u001b[32m1.1780\u001b[0m       \u001b[35m0.4271\u001b[0m        \u001b[31m1.2497\u001b[0m     +  0.0100  0.1172\n",
      "      5       \u001b[36m0.5486\u001b[0m        \u001b[32m1.1128\u001b[0m       \u001b[35m0.4688\u001b[0m        \u001b[31m1.2172\u001b[0m     +  0.0100  0.1175\n",
      "      6       \u001b[36m0.5694\u001b[0m        \u001b[32m1.0116\u001b[0m       \u001b[35m0.4826\u001b[0m        \u001b[31m1.2103\u001b[0m     +  0.0099  0.1182\n",
      "      7       \u001b[36m0.6076\u001b[0m        \u001b[32m0.9260\u001b[0m       \u001b[35m0.5035\u001b[0m        \u001b[31m1.1957\u001b[0m     +  0.0099  0.1176\n",
      "      8       \u001b[36m0.6632\u001b[0m        \u001b[32m0.8628\u001b[0m       \u001b[35m0.5382\u001b[0m        \u001b[31m1.1804\u001b[0m     +  0.0099  0.1191\n",
      "      9       \u001b[36m0.6667\u001b[0m        \u001b[32m0.8079\u001b[0m       0.4965        1.2901        0.0098  0.1179\n",
      "     10       \u001b[36m0.6701\u001b[0m        \u001b[32m0.7476\u001b[0m       \u001b[35m0.5451\u001b[0m        1.2194        0.0098  0.1177\n",
      "     11       \u001b[36m0.7361\u001b[0m        \u001b[32m0.7061\u001b[0m       0.5174        1.2899        0.0098  0.1173\n",
      "     12       \u001b[36m0.7396\u001b[0m        \u001b[32m0.6921\u001b[0m       0.4688        1.4926        0.0097  0.1172\n",
      "     13       0.7257        0.7068       0.5174        1.2217        0.0096  0.1185\n",
      "     14       \u001b[36m0.7465\u001b[0m        \u001b[32m0.6392\u001b[0m       0.5035        1.2796        0.0096  0.1180\n",
      "     15       \u001b[36m0.7500\u001b[0m        \u001b[32m0.6201\u001b[0m       0.4931        1.2709        0.0095  0.1181\n",
      "     16       \u001b[36m0.7882\u001b[0m        \u001b[32m0.5674\u001b[0m       0.5035        1.2446        0.0094  0.1175\n",
      "     17       0.7743        \u001b[32m0.5554\u001b[0m       0.5104        1.2556        0.0094  0.1181\n",
      "     18       \u001b[36m0.8056\u001b[0m        \u001b[32m0.4999\u001b[0m       0.4757        1.3517        0.0093  0.1183\n",
      "     19       0.7396        0.5770       0.5035        1.2519        0.0092  0.1174\n",
      "     20       0.7778        0.5427       \u001b[35m0.5556\u001b[0m        \u001b[31m1.1376\u001b[0m     +  0.0091  0.1160\n",
      "     21       \u001b[36m0.8368\u001b[0m        \u001b[32m0.4410\u001b[0m       0.5243        1.2142        0.0090  0.1197\n",
      "     22       0.8264        \u001b[32m0.4263\u001b[0m       0.4826        1.5299        0.0089  0.1174\n",
      "     23       0.8056        0.4711       0.4757        1.5125        0.0088  0.1174\n",
      "     24       0.8125        0.4502       0.5208        1.3943        0.0087  0.1188\n",
      "     25       \u001b[36m0.8438\u001b[0m        0.4383       0.4653        1.6041        0.0086  0.1196\n",
      "     26       0.8368        0.4487       0.4757        1.5277        0.0085  0.1171\n",
      "     27       \u001b[36m0.8542\u001b[0m        \u001b[32m0.3843\u001b[0m       \u001b[35m0.5590\u001b[0m        1.2024        0.0084  0.1176\n",
      "     28       0.8438        0.4210       0.5556        1.1680        0.0083  0.1197\n",
      "     29       0.8507        0.4061       0.5312        1.2237        0.0082  0.1180\n",
      "     30       0.8438        0.4049       0.5278        1.1679        0.0080  0.1202\n",
      "     31       0.8542        0.3865       \u001b[35m0.5799\u001b[0m        \u001b[31m1.1026\u001b[0m     +  0.0079  0.1192\n",
      "     32       \u001b[36m0.8958\u001b[0m        \u001b[32m0.3514\u001b[0m       \u001b[35m0.5868\u001b[0m        1.1478        0.0078  0.1204\n",
      "     33       0.8472        0.3752       0.5764        1.1416        0.0076  0.1212\n",
      "     34       0.8715        0.3657       \u001b[35m0.5903\u001b[0m        \u001b[31m1.0391\u001b[0m     +  0.0075  0.1185\n",
      "     35       0.8681        \u001b[32m0.3180\u001b[0m       \u001b[35m0.6007\u001b[0m        \u001b[31m1.0090\u001b[0m     +  0.0074  0.1193\n",
      "     36       0.8611        \u001b[32m0.3042\u001b[0m       0.5903        1.1322        0.0072  0.1181\n",
      "     37       0.8403        0.3829       0.5764        1.2287        0.0071  0.1176\n",
      "     38       0.8715        0.3082       0.5625        1.1329        0.0069  0.1182\n",
      "     39       0.8472        0.3630       0.5451        1.2803        0.0068  0.1199\n",
      "     40       0.8924        0.3188       0.5139        1.4849        0.0066  0.1176\n",
      "     41       0.8750        \u001b[32m0.2900\u001b[0m       0.5347        1.5554        0.0065  0.1236\n",
      "     42       0.8889        0.3009       0.5556        1.3574        0.0063  0.1182\n",
      "     43       0.8646        0.3050       0.5903        1.1744        0.0062  0.1184\n",
      "     44       \u001b[36m0.9236\u001b[0m        \u001b[32m0.2314\u001b[0m       0.5799        1.2273        0.0060  0.1184\n",
      "     45       0.9062        0.2685       0.5764        1.2388        0.0059  0.1174\n",
      "     46       0.8819        0.3224       0.5625        1.3293        0.0057  0.1178\n",
      "     47       0.9097        0.2583       0.5729        1.2698        0.0056  0.1188\n",
      "     48       0.9028        0.2515       0.5729        1.2457        0.0054  0.1171\n",
      "     49       0.9167        0.2619       0.5660        1.2359        0.0052  0.1193\n",
      "     50       0.8993        0.2415       0.5660        1.2061        0.0051  0.1181\n",
      "     51       0.9132        \u001b[32m0.2244\u001b[0m       0.5972        1.1768        0.0049  0.1193\n",
      "     52       0.9236        \u001b[32m0.2115\u001b[0m       0.5729        1.2590        0.0048  0.1187\n",
      "     53       0.9097        0.2425       0.5694        1.2260        0.0046  0.1177\n",
      "     54       \u001b[36m0.9306\u001b[0m        \u001b[32m0.1987\u001b[0m       0.5625        1.2772        0.0044  0.1190\n",
      "     55       \u001b[36m0.9410\u001b[0m        0.2100       0.5660        1.3100        0.0043  0.1199\n",
      "     56       \u001b[36m0.9514\u001b[0m        \u001b[32m0.1933\u001b[0m       0.5868        1.2552        0.0041  0.1185\n",
      "     57       0.9062        0.2449       0.5833        1.2274        0.0040  0.1187\n",
      "     58       0.8993        0.2646       \u001b[35m0.6285\u001b[0m        1.1188        0.0038  0.1184\n",
      "     59       0.9340        \u001b[32m0.1777\u001b[0m       0.6215        1.1373        0.0037  0.1203\n",
      "     60       0.9201        0.2353       \u001b[35m0.6493\u001b[0m        1.0787        0.0035  0.1276\n",
      "     61       0.9062        0.2368       0.6389        1.1009        0.0034  0.1184\n",
      "     62       0.9340        0.2035       0.6319        1.1675        0.0032  0.1176\n",
      "     63       0.9375        0.2021       0.6250        1.2050        0.0031  0.1189\n",
      "     64       0.9236        0.2059       0.6285        1.1983        0.0029  0.1174\n",
      "     65       0.9375        0.2154       0.6493        1.1294        0.0028  0.1173\n",
      "     66       0.9236        0.1823       0.6493        1.1412        0.0026  0.1180\n",
      "     67       0.9340        0.1828       0.6215        1.2435        0.0025  0.1175\n",
      "     68       0.9271        0.1916       0.6215        1.2365        0.0024  0.1187\n",
      "     69       0.9236        0.1958       0.6493        1.1592        0.0022  0.1190\n",
      "     70       \u001b[36m0.9549\u001b[0m        \u001b[32m0.1407\u001b[0m       0.6493        1.1282        0.0021  0.1175\n",
      "     71       0.9514        0.1651       \u001b[35m0.6840\u001b[0m        1.0198        0.0020  0.1191\n",
      "     72       0.9028        0.2239       \u001b[35m0.6910\u001b[0m        \u001b[31m0.9852\u001b[0m     +  0.0018  0.1167\n",
      "     73       0.9444        0.1827       0.6910        \u001b[31m0.9835\u001b[0m     +  0.0017  0.1176\n",
      "     74       0.9201        0.2162       0.6736        1.0432        0.0016  0.1195\n",
      "     75       0.9340        0.1802       0.6667        1.0497        0.0015  0.1178\n",
      "     76       0.9444        0.1835       0.6632        1.0750        0.0014  0.1157\n",
      "     77       0.9479        0.1570       0.6562        1.1280        0.0013  0.1164\n",
      "     78       0.9514        0.1538       0.6528        1.1440        0.0012  0.1173\n",
      "     79       0.9306        0.1958       0.6424        1.1925        0.0011  0.1182\n",
      "     80       0.9132        0.2340       0.6389        1.2117        0.0010  0.1162\n",
      "     81       0.9271        0.2019       0.6285        1.2333        0.0009  0.1161\n",
      "     82       \u001b[36m0.9583\u001b[0m        \u001b[32m0.1341\u001b[0m       0.6354        1.1919        0.0008  0.1169\n",
      "     83       0.9097        0.2350       0.6354        1.2111        0.0007  0.1174\n",
      "     84       \u001b[36m0.9618\u001b[0m        0.1426       0.6424        1.1780        0.0006  0.1175\n",
      "     85       0.9097        0.2159       0.6493        1.1531        0.0006  0.1159\n",
      "     86       0.9583        0.1466       0.6458        1.1638        0.0005  0.1170\n",
      "     87       0.9444        0.1541       0.6562        1.1549        0.0004  0.1178\n",
      "     88       0.9375        0.1714       0.6632        1.1429        0.0004  0.1190\n",
      "     89       0.9375        0.1600       0.6667        1.1212        0.0003  0.1160\n",
      "     90       0.9514        0.1447       0.6632        1.1268        0.0002  0.1173\n",
      "     91       0.9167        0.2003       0.6562        1.1421        0.0002  0.1188\n",
      "     92       0.9340        0.1972       0.6562        1.1506        0.0002  0.1175\n",
      "     93       0.9271        0.1844       0.6562        1.1516        0.0001  0.1176\n",
      "     94       0.9549        0.1467       0.6562        1.1448        0.0001  0.1164\n",
      "     95       0.9340        0.2155       0.6562        1.1365        0.0001  0.1162\n",
      "     96       0.9514        0.1369       0.6562        1.1368        0.0000  0.1181\n",
      "     97       0.9340        0.1683       0.6562        1.1382        0.0000  0.1184\n",
      "     98       0.9444        0.1622       0.6493        1.1567        0.0000  0.1191\n",
      "     99       0.9271        0.1809       0.6562        1.1491        0.0000  0.1186\n",
      "    100       0.9514        0.1688       0.6528        1.1487        0.0000  0.1185\n"
     ]
    }
   ],
   "source": [
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = EEGNet(n_channels, n_samples, n_classes,\n",
    "            time_kernel=(8, (1, 64), (1, 1)),\n",
    "            D=2,\n",
    "            pool_kernel1=((1, 4), (1, 4)),\n",
    "            separa_kernel=(16, (1, 16), (1, 1)),\n",
    "            pool_kernel2=((1, 8), (1, 8)),\n",
    "            depthwise_norm_rate=1,\n",
    "            fc_norm_rate=0.25,\n",
    "            dropout_rate=0.5)\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.CrossEntropyLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ckp = Checkpoint(dirname=ckp_dirname)\n",
    "train_end_ckp = TrainEndCheckpoint(dirname=ckp_dirname)\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        validX, validY))\n",
    "net = NeuralNetClassifier(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            ('train_acc', EpochScoring('accuracy', \n",
    "                                    name='train_acc', \n",
    "                                    on_train=True, \n",
    "                                    lower_is_better=False)),\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "            ckp,\n",
    "            train_end_ckp\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    trainX, y=trainY, epochs=max_epochs)\n",
    "net.load_params(checkpoint=ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_acc    train_loss    valid_acc    valid_loss    cp      lr     dur\n",
      "-------  -----------  ------------  -----------  ------------  ----  ------  ------\n",
      "      1       \u001b[36m0.2986\u001b[0m        \u001b[32m1.9316\u001b[0m       \u001b[35m0.3090\u001b[0m        \u001b[31m2.0845\u001b[0m     +  0.0010  3.1516\n",
      "      2       \u001b[36m0.4271\u001b[0m        \u001b[32m1.5461\u001b[0m       0.2812        2.5431        0.0010  2.1912\n",
      "      3       \u001b[36m0.4549\u001b[0m        \u001b[32m1.2975\u001b[0m       \u001b[35m0.3715\u001b[0m        \u001b[31m1.9438\u001b[0m     +  0.0010  2.2093\n",
      "      4       \u001b[36m0.5938\u001b[0m        \u001b[32m0.9878\u001b[0m       \u001b[35m0.3889\u001b[0m        2.0292        0.0010  2.1937\n",
      "      5       \u001b[36m0.6493\u001b[0m        \u001b[32m0.8703\u001b[0m       \u001b[35m0.4097\u001b[0m        \u001b[31m1.7038\u001b[0m     +  0.0010  2.2328\n",
      "      6       \u001b[36m0.7083\u001b[0m        \u001b[32m0.7619\u001b[0m       \u001b[35m0.4236\u001b[0m        1.7468        0.0010  2.2357\n",
      "      7       \u001b[36m0.7535\u001b[0m        \u001b[32m0.5907\u001b[0m       0.3993        1.8018        0.0010  2.2300\n",
      "      8       \u001b[36m0.7917\u001b[0m        \u001b[32m0.5575\u001b[0m       0.4236        1.7799        0.0010  2.2154\n",
      "      9       \u001b[36m0.8576\u001b[0m        \u001b[32m0.4292\u001b[0m       0.4201        1.8375        0.0010  2.2094\n",
      "     10       \u001b[36m0.8750\u001b[0m        \u001b[32m0.3498\u001b[0m       0.4132        1.9134        0.0010  2.2157\n",
      "     11       0.8611        \u001b[32m0.3341\u001b[0m       \u001b[35m0.4306\u001b[0m        1.9565        0.0010  2.2071\n",
      "     12       \u001b[36m0.9410\u001b[0m        \u001b[32m0.2437\u001b[0m       0.4271        2.0070        0.0010  2.2568\n",
      "     13       \u001b[36m0.9618\u001b[0m        \u001b[32m0.1996\u001b[0m       0.4201        1.9712        0.0010  2.2558\n",
      "     14       0.9618        \u001b[32m0.1837\u001b[0m       \u001b[35m0.4583\u001b[0m        1.9205        0.0010  2.1993\n",
      "     15       \u001b[36m0.9722\u001b[0m        \u001b[32m0.1576\u001b[0m       \u001b[35m0.4653\u001b[0m        1.9579        0.0010  2.2110\n",
      "     16       \u001b[36m0.9792\u001b[0m        \u001b[32m0.1355\u001b[0m       0.4444        1.9895        0.0009  2.2148\n",
      "     17       \u001b[36m0.9931\u001b[0m        \u001b[32m0.1182\u001b[0m       0.4306        2.0364        0.0009  2.2139\n",
      "     18       0.9826        \u001b[32m0.1051\u001b[0m       0.4271        2.0608        0.0009  2.2101\n",
      "     19       0.9896        \u001b[32m0.0925\u001b[0m       0.4306        2.0846        0.0009  2.2058\n",
      "     20       \u001b[36m1.0000\u001b[0m        \u001b[32m0.0688\u001b[0m       0.4201        2.0980        0.0009  2.2071\n",
      "     21       0.9965        0.0786       0.4375        2.0944        0.0009  2.2072\n",
      "     22       1.0000        \u001b[32m0.0593\u001b[0m       0.4271        2.1217        0.0009  2.2130\n",
      "     23       0.9965        \u001b[32m0.0590\u001b[0m       0.4340        2.1763        0.0009  2.2069\n",
      "     24       1.0000        \u001b[32m0.0458\u001b[0m       0.4410        2.1846        0.0009  2.2065\n",
      "     25       0.9931        0.0475       0.4410        2.2175        0.0009  2.2063\n",
      "     26       0.9965        \u001b[32m0.0446\u001b[0m       0.4271        2.2651        0.0009  2.1997\n",
      "     27       1.0000        \u001b[32m0.0383\u001b[0m       0.4375        2.2285        0.0008  2.2121\n",
      "     28       1.0000        0.0387       0.4340        2.2294        0.0008  2.2095\n",
      "     29       1.0000        \u001b[32m0.0332\u001b[0m       0.4375        2.2284        0.0008  2.2068\n",
      "     30       0.9931        0.0375       0.4444        2.2214        0.0008  2.2360\n",
      "     31       0.9965        0.0334       0.4375        2.2568        0.0008  2.2131\n",
      "     32       1.0000        \u001b[32m0.0276\u001b[0m       0.4271        2.2938        0.0008  2.2229\n",
      "     33       1.0000        \u001b[32m0.0251\u001b[0m       0.4271        2.3014        0.0008  2.2253\n",
      "     34       0.9965        0.0286       0.4306        2.3078        0.0008  2.2155\n",
      "     35       0.9965        0.0323       0.4479        2.3016        0.0007  2.2164\n",
      "     36       1.0000        \u001b[32m0.0229\u001b[0m       0.4549        2.3171        0.0007  2.2368\n",
      "     37       1.0000        0.0310       0.4549        2.3146        0.0007  2.2183\n",
      "     38       1.0000        \u001b[32m0.0207\u001b[0m       0.4410        2.3254        0.0007  2.2138\n",
      "     39       1.0000        0.0229       0.4479        2.3236        0.0007  2.2041\n",
      "     40       1.0000        0.0247       0.4479        2.3228        0.0007  2.2239\n",
      "     41       1.0000        \u001b[32m0.0181\u001b[0m       0.4479        2.3333        0.0006  2.2230\n",
      "     42       0.9965        0.0214       0.4375        2.3409        0.0006  2.2195\n",
      "     43       1.0000        0.0236       0.4306        2.3690        0.0006  2.2349\n",
      "     44       1.0000        \u001b[32m0.0156\u001b[0m       0.4271        2.3577        0.0006  2.2086\n",
      "     45       1.0000        0.0166       0.4410        2.3720        0.0006  2.2237\n",
      "     46       1.0000        0.0196       0.4410        2.3934        0.0006  2.2293\n",
      "     47       1.0000        \u001b[32m0.0131\u001b[0m       0.4479        2.4213        0.0006  2.2078\n",
      "     48       1.0000        0.0160       0.4479        2.4236        0.0005  2.2248\n",
      "     49       1.0000        0.0144       0.4514        2.4420        0.0005  2.2120\n",
      "     50       1.0000        0.0162       0.4479        2.4295        0.0005  2.2059\n",
      "     51       1.0000        \u001b[32m0.0114\u001b[0m       0.4479        2.4366        0.0005  2.2029\n",
      "     52       1.0000        0.0139       0.4514        2.4237        0.0005  2.2143\n",
      "     53       1.0000        \u001b[32m0.0109\u001b[0m       0.4479        2.4397        0.0005  2.2051\n",
      "     54       1.0000        0.0150       0.4410        2.4545        0.0004  2.2055\n",
      "     55       1.0000        0.0135       0.4444        2.4546        0.0004  2.2189\n",
      "     56       1.0000        0.0123       0.4410        2.4616        0.0004  2.2065\n",
      "     57       1.0000        \u001b[32m0.0087\u001b[0m       0.4444        2.4696        0.0004  2.2082\n",
      "     58       1.0000        0.0094       0.4375        2.4741        0.0004  2.2111\n",
      "     59       1.0000        0.0141       0.4340        2.4660        0.0004  2.2026\n",
      "     60       1.0000        0.0106       0.4306        2.4657        0.0004  2.2048\n",
      "     61       1.0000        0.0094       0.4375        2.4771        0.0003  2.2069\n",
      "     62       1.0000        0.0101       0.4410        2.5045        0.0003  2.2057\n",
      "     63       0.9965        0.0138       0.4375        2.5080        0.0003  2.2018\n",
      "     64       1.0000        0.0162       0.4375        2.5081        0.0003  2.2024\n",
      "     65       1.0000        0.0103       0.4375        2.4814        0.0003  2.2100\n",
      "     66       1.0000        0.0103       0.4410        2.4651        0.0003  2.2132\n",
      "     67       1.0000        0.0096       0.4410        2.4705        0.0003  2.2073\n",
      "     68       1.0000        0.0123       0.4479        2.4667        0.0002  2.2387\n",
      "     69       1.0000        0.0088       0.4514        2.4584        0.0002  2.2301\n",
      "     70       1.0000        0.0087       0.4514        2.4519        0.0002  2.2454\n",
      "     71       1.0000        0.0096       0.4479        2.4660        0.0002  2.2518\n",
      "     72       1.0000        0.0106       0.4514        2.4680        0.0002  2.2449\n",
      "     73       1.0000        0.0093       0.4479        2.4728        0.0002  2.2123\n",
      "     74       1.0000        \u001b[32m0.0076\u001b[0m       0.4479        2.4626        0.0002  2.2230\n",
      "     75       1.0000        0.0085       0.4514        2.4615        0.0001  2.2504\n",
      "     76       1.0000        0.0077       0.4444        2.4663        0.0001  2.2429\n",
      "     77       1.0000        0.0091       0.4444        2.4535        0.0001  2.2205\n",
      "     78       1.0000        0.0101       0.4410        2.4722        0.0001  2.2242\n",
      "     79       1.0000        \u001b[32m0.0075\u001b[0m       0.4444        2.4788        0.0001  2.2323\n",
      "     80       1.0000        0.0084       0.4444        2.4855        0.0001  2.2089\n",
      "     81       1.0000        \u001b[32m0.0074\u001b[0m       0.4444        2.4898        0.0001  2.2042\n",
      "     82       1.0000        0.0097       0.4444        2.4835        0.0001  2.2048\n",
      "     83       1.0000        0.0083       0.4444        2.4879        0.0001  2.2030\n",
      "     84       1.0000        0.0088       0.4444        2.5057        0.0001  2.2129\n",
      "     85       1.0000        0.0084       0.4444        2.5114        0.0001  2.2213\n",
      "     86       1.0000        \u001b[32m0.0074\u001b[0m       0.4410        2.5266        0.0000  2.2228\n",
      "     87       1.0000        0.0090       0.4410        2.5282        0.0000  2.1966\n",
      "     88       1.0000        0.0077       0.4444        2.5230        0.0000  2.2113\n",
      "     89       1.0000        0.0078       0.4444        2.5089        0.0000  2.2087\n",
      "     90       1.0000        \u001b[32m0.0067\u001b[0m       0.4410        2.5142        0.0000  2.2108\n",
      "     91       1.0000        0.0075       0.4410        2.5065        0.0000  2.2100\n",
      "     92       1.0000        0.0081       0.4444        2.5012        0.0000  2.2186\n",
      "     93       1.0000        0.0101       0.4444        2.5045        0.0000  2.2011\n",
      "     94       1.0000        0.0079       0.4444        2.4935        0.0000  2.2173\n",
      "     95       1.0000        0.0098       0.4444        2.5202        0.0000  2.2209\n",
      "     96       1.0000        0.0090       0.4444        2.5257        0.0000  2.2233\n",
      "     97       1.0000        0.0114       0.4444        2.5108        0.0000  2.2056\n",
      "     98       0.9965        0.0116       0.4444        2.5045        0.0000  2.2171\n",
      "     99       1.0000        0.0071       0.4444        2.5127        0.0000  2.2115\n",
      "    100       1.0000        0.0072       0.4444        2.5155        0.0000  2.2073\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetClassifierNoLog(NeuralNetClassifier):\n",
    "    def get_loss(self, y_pred, y_true, *args, **kwargs):\n",
    "        return super(NeuralNetClassifier, self).get_loss(y_pred, y_true, *args, **kwargs)\n",
    "\n",
    "set_random_seeds(20200220)\n",
    "n_trials, n_channels, n_samples = X.shape\n",
    "labels =  np.unique(y)\n",
    "n_classes = len(labels)\n",
    "\n",
    "model = TIDNet(n_channels, n_classes, \n",
    "            input_window_samples=n_samples,\n",
    "            s_growth=24,\n",
    "            t_filters=32, \n",
    "            temp_layers=2, \n",
    "            spat_layers=2, \n",
    "            pooling=15, \n",
    "            temp_span=0.05, \n",
    "            bottleneck=3)\n",
    "\n",
    "initial_state = copy.deepcopy(model.state_dict())\n",
    "criterion = nn.NLLLoss\n",
    "optimizer = optim.AdamW\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "max_epochs = 100\n",
    "\n",
    "ckp_dirname = 'runs_training_pipeline'\n",
    "verbose = True\n",
    "\n",
    "# train set, session_T\n",
    "ind = meta[meta['session']=='session_1'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "trainX, trainY = np.copy(X[ind]), np.copy(y[ind])\n",
    "# test set, session_E\n",
    "ind = meta[meta['session']=='session_0'].sort_values(by=['session', 'run', 'trial_id']).index\n",
    "validX, validY = np.copy(X[ind]), np.copy(y[ind])\n",
    "\n",
    "trainX, validX = generate_tensors(trainX, validX, dtype=torch.float)\n",
    "trainY, validY = generate_tensors(trainY, validY, dtype=torch.long)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ckp = Checkpoint(dirname=ckp_dirname)\n",
    "train_end_ckp = TrainEndCheckpoint(dirname=ckp_dirname)\n",
    "\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "train_split = predefined_split(\n",
    "    skorch.dataset.Dataset(\n",
    "        validX, validY))\n",
    "net = NeuralNetClassifierNoLog(model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        optimizer__lr=lr,\n",
    "        optimizer__weight_decay=0,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        train_split=train_split,\n",
    "        iterator_train__shuffle=True,\n",
    "        callbacks=[\n",
    "            ('train_acc', EpochScoring('accuracy', \n",
    "                                    name='train_acc', \n",
    "                                    on_train=True, \n",
    "                                    lower_is_better=False)),\n",
    "            ('lr_scheduler', LRScheduler('CosineAnnealingLR', T_max=max_epochs - 1)),\n",
    "            ckp,\n",
    "            train_end_ckp\n",
    "        ],\n",
    "        verbose=verbose)\n",
    "net = net.fit(\n",
    "    trainX, y=trainY, epochs=max_epochs)\n",
    "net.load_params(checkpoint=ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
