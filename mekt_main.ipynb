{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, copy, os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "from brainda.datasets import (\n",
    "    PhysionetMI, BNCI2014001, Weibo2014, Cho2017)\n",
    "\n",
    "from brainda.paradigms import MotorImagery\n",
    "from brainda.algorithms.utils.model_selection import (\n",
    "    set_random_seeds,\n",
    "    generate_kfold_indices, match_kfold_indices)\n",
    "\n",
    "from brainda.algorithms.transfer_learning import MEKT, choose_multiple_subjects\n",
    "from brainda.algorithms.manifold import tangent_space, mean_riemann\n",
    "from brainda.algorithms.utils.covariance import Covariance, invsqrtm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_hook(raw, caches, verbose=False):\n",
    "    raw.filter(4, 40, l_trans_bandwidth=2, h_trans_bandwidth=5, phase='zero-double')\n",
    "    return raw, caches\n",
    "\n",
    "def mekt_transform_and_predict(\n",
    "    source_dataset, target_dataset, selected_channels, srate, duration, events,\n",
    "    kfold=5, \n",
    "    save_folder='mekt',\n",
    "    force_update=False):\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    save_file = \"{}->{}-mekt-{}classes.joblib\".format(\n",
    "        source_dataset.dataset_code, \n",
    "        target_dataset.dataset_code, \n",
    "        len(events))\n",
    "    \n",
    "    if not force_update and os.path.exists(os.path.join(save_folder, save_file)):\n",
    "        kfold_accs = joblib.load(os.path.join(save_folder, save_file))['kfold_accs']\n",
    "        print(\"Source: {} Target: {} {} Acc: {:.4f}\".format(\n",
    "            source_dataset.dataset_code,\n",
    "            target_dataset.dataset_code,\n",
    "            'mekt',\n",
    "            np.mean(kfold_accs)))\n",
    "        return\n",
    "    \n",
    "    start_t = source_dataset.events['left_hand'][1][0]\n",
    "    if start_t+ duration > source_dataset.events['left_hand'][1][1]:\n",
    "        print(\"Warning: the current dataset avaliable trial duration is not long enough.\")\n",
    "    events = ['left_hand', 'right_hand']\n",
    "    paradigm = MotorImagery(\n",
    "        channels=selected_channels, \n",
    "        srate=srate, \n",
    "        intervals=[(start_t, start_t + duration)], \n",
    "        events=events)\n",
    "\n",
    "    event_id = [source_dataset.events[e][0] for e in events]\n",
    "    paradigm.register_raw_hook(raw_hook)\n",
    "    Xs, ys, metas = paradigm.get_data(\n",
    "        source_dataset, \n",
    "        subjects=source_dataset.subjects, \n",
    "        return_concat=True, \n",
    "        verbose=False)\n",
    "    ys = label_encoder(ys, event_id)\n",
    "    \n",
    "    set_random_seeds(38)\n",
    "    indices = generate_kfold_indices(metas, kfold=kfold)\n",
    "    \n",
    "    kfold_accs, kfold_cms = [], []\n",
    "    set_random_seeds(42)\n",
    "\n",
    "    for k in range(kfold):\n",
    "        featureXs, featureYs, featureSubs = [], [], []    \n",
    "        filterX, filterY = np.copy(Xs), np.copy(ys)\n",
    "        filterX = filterX - np.mean(filterX, axis=-1, keepdims=True)\n",
    "        covest = Covariance(estimator='lwf')\n",
    "        filterX = covest.transform(filterX)\n",
    "        subjects = metas['subject'].to_numpy()\n",
    "        for sub_id in source_dataset.subjects:\n",
    "            sub_meta = metas[metas['subject']==sub_id]\n",
    "            train_ind, validate_ind, test_ind = match_kfold_indices(k, sub_meta, indices)\n",
    "            train_ind = np.concatenate((train_ind, validate_ind))\n",
    "            M = mean_riemann(filterX[train_ind])\n",
    "            iM12 = invsqrtm(M)\n",
    "            Cs = iM12@filterX[train_ind]@iM12.T\n",
    "            featureXs.append(tangent_space(Cs, np.eye(M.shape[0])))\n",
    "            featureYs.append(filterY[train_ind])\n",
    "            featureSubs.append(subjects[train_ind])\n",
    "        featureXs = np.concatenate(featureXs, axis=0)\n",
    "        featureYs = np.concatenate(featureYs, axis=0)\n",
    "        featureSubs = np.concatenate(featureSubs, axis=0)\n",
    "\n",
    "        sub_accs, sub_cms = [], []\n",
    "        if source_dataset.dataset_code != target_dataset.dataset_code:\n",
    "            for sub_id in target_dataset.subjects:\n",
    "                start_t = target_dataset.events['left_hand'][1][0]\n",
    "                if start_t+ duration > target_dataset.events['left_hand'][1][1]:\n",
    "                    print(\"Warning: the current dataset avaliable trial duration is not long enough.\")\n",
    "                paradigm = MotorImagery(\n",
    "                    channels=selected_channels, \n",
    "                    srate=srate, \n",
    "                    intervals=[(start_t, start_t + duration)],\n",
    "                    events=events)\n",
    "                event_id = [target_dataset.events[e][0] for e in events]\n",
    "\n",
    "                paradigm.register_raw_hook(raw_hook)\n",
    "                Xt, yt, metat = paradigm.get_data(target_dataset, subjects=[sub_id], return_concat=True, verbose=False)\n",
    "                yt = label_encoder(yt, event_id)\n",
    "\n",
    "                Xt = covest.transform(Xt)\n",
    "                M = mean_riemann(Xt)\n",
    "                iM12 = invsqrtm(M)\n",
    "                Ct = iM12@Xt@iM12.T\n",
    "                featureXt = tangent_space(Ct, np.eye(M.shape[0]))\n",
    "                mekt = MEKT()\n",
    "                source_features, target_features = mekt.fit_transform(featureXs, featureYs, featureXt)\n",
    "                clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "                pred_labels = clf.fit(source_features, featureYs).predict(target_features)\n",
    "                true_labels = yt\n",
    "                sub_accs.append(balanced_accuracy_score(true_labels, pred_labels))\n",
    "                cm = confusion_matrix(true_labels, pred_labels, labels=np.unique(true_labels), normalize='true')\n",
    "                sub_cms.append(cm)\n",
    "        else:\n",
    "            # leave-one-out scheme\n",
    "            for sub_id in target_dataset.subjects:\n",
    "                start_t = target_dataset.events['left_hand'][1][0]\n",
    "                if start_t+ duration > target_dataset.events['left_hand'][1][1]:\n",
    "                    print(\"Warning: the current dataset avaliable trial duration is not long enough.\")\n",
    "                paradigm = MotorImagery(\n",
    "                    channels=selected_channels, \n",
    "                    srate=srate, \n",
    "                    intervals=[(start_t, start_t + duration)],\n",
    "                    events=events)\n",
    "                event_id = [target_dataset.events[e][0] for e in events]\n",
    "\n",
    "                paradigm.register_raw_hook(raw_hook)\n",
    "                Xt, yt, metat = paradigm.get_data(target_dataset, subjects=[sub_id], return_concat=True, verbose=False)\n",
    "                yt = label_encoder(yt, event_id)\n",
    "\n",
    "                Xt = covest.transform(Xt)\n",
    "                M = mean_riemann(Xt)\n",
    "                iM12 = invsqrtm(M)\n",
    "                Ct = iM12@Xt@iM12.T\n",
    "                featureXt = tangent_space(Ct, np.eye(M.shape[0]))\n",
    "                mekt = MEKT()\n",
    "                \n",
    "                source_features, target_features = mekt.fit_transform(\n",
    "                    featureXs[featureSubs!=sub_id], featureYs[featureSubs!=sub_id], featureXt)\n",
    "                clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "\n",
    "                pred_labels = clf.fit(source_features, featureYs[featureSubs!=sub_id]).predict(target_features)\n",
    "                true_labels = yt\n",
    "                sub_accs.append(balanced_accuracy_score(true_labels, pred_labels))\n",
    "                cm = confusion_matrix(true_labels, pred_labels, labels=np.unique(true_labels), normalize='true')\n",
    "                sub_cms.append(cm)\n",
    "            \n",
    "        kfold_accs.append(sub_accs)\n",
    "        kfold_cms.append(sub_cms)\n",
    "    print(\"Source: {} Target: {} {} Acc: {:.4f}\".format(\n",
    "        source_dataset.dataset_code,\n",
    "        target_dataset.dataset_code,\n",
    "        'mekt',\n",
    "        np.mean(kfold_accs)))\n",
    "    joblib.dump(\n",
    "        {'kfold_accs': kfold_accs, \"kfold_cms\": kfold_cms}, \n",
    "        os.path.join(save_folder, save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: bnci2014001 Target: bnci2014001 mekt Acc: 0.6979\n",
      "Source: bnci2014001 Target: eegbci mekt Acc: 0.6358\n",
      "Source: bnci2014001 Target: weibo2014 mekt Acc: 0.6863\n",
      "Source: bnci2014001 Target: cho2017 mekt Acc: 0.5885\n",
      "Source: eegbci Target: bnci2014001 mekt Acc: 0.7063\n",
      "Source: eegbci Target: eegbci mekt Acc: 0.6640\n",
      "Source: eegbci Target: weibo2014 mekt Acc: 0.6988\n",
      "Source: eegbci Target: cho2017 mekt Acc: 0.6030\n",
      "Source: weibo2014 Target: bnci2014001 mekt Acc: 0.6843\n",
      "Source: weibo2014 Target: eegbci mekt Acc: 0.6372\n",
      "Source: weibo2014 Target: weibo2014 mekt Acc: 0.6997\n"
     ]
    }
   ],
   "source": [
    "srate = 128\n",
    "datasets = [BNCI2014001(), PhysionetMI(), Weibo2014(), Cho2017()]\n",
    "\n",
    "selected_channels = [\n",
    "    'FZ', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', \n",
    "    'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', \n",
    "    'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', \n",
    "    'P1', 'PZ', 'P2', 'POZ']\n",
    "duration = 3\n",
    "events = ['left_hand', 'right_hand']\n",
    "kfold = 5\n",
    "save_folder = 'mekt'\n",
    "\n",
    "for source_dataset in datasets:\n",
    "    for target_dataset in datasets:\n",
    "        mekt_transform_and_predict(\n",
    "            source_dataset, target_dataset, selected_channels, srate, duration, events,\n",
    "            save_folder=save_folder,\n",
    "            force_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
